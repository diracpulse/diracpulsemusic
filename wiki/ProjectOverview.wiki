#summary Project Overview

Humans perceive sound in 3 dimensions: time, frequency, and amplitude (referred to as _TFA_ in this project).  Musical notation is actually TFA data: a range of notes (i.e. frequencies) of a particular duration (_ex. quarter notes_), sometimes with specified loudness (_ex. forte or pianissimo_).  However, sound is recorded in 2 dimensions: time and amplitude (in digital. _PCM _(_Pulse Code Modulation_)).  Modern file compression schemes such as used in the MP3 do translate PCM data into banks of frequencies.  This is done in order to remove frequencies that are inaudible at a particular time.  This reduces the amount of data needed to save audio data.  However, the frequency data is a "block box".  Uncompressed PCM data is saved into an MP3 which is simply converted back to PCM data.  The frequency data is not accessible.  Even if it was, the MP3 frequency domain representation is not a good representation of what is actually heard.  This project uses a computationally expensive, but psychoacoustically accurate algorithm to convert PCM data into TFA data actually heard by humans.  Also, unlike file compression schemes like MP3, there is no algorithm (that I know of) for converting TFA data back into PCM data.  A Java GUI is used to view the TFA data, and select the audible harmonics.  The selected harmonics are then converted into PCM data for playback.  The advantage of using a 3 dimensional representation is that all possible sounds can be directly synthesized using this approach without needing large numbers of specialized plug-ins for synthesis.  Also, equalization can be applied perfectly to TFA data.  The time signatures, chords, instrument placement, etc can all be modified in TFA data without artifacts _(at least in theory)_.  It is the goal of this project to realize these theoretical possibilities.